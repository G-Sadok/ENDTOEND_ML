# üìä Reddit Real-Time Sentiment Analysis Pipeline

![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python)
![Apache Kafka](https://img.shields.io/badge/Apache_Kafka-Streaming-231F20?style=for-the-badge&logo=apachekafka&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Database-336791?style=for-the-badge&logo=postgresql&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-ML-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)

## üìñ Aper√ßu du Projet

Ce projet met en ≈ìuvre une **architecture de streaming de donn√©es** compl√®te pour analyser en temps r√©el le sentiment des discussions sur **Reddit**.

L'objectif est de capter le flux de commentaires, de pr√©dire instantan√©ment si le ton est positif ou n√©gatif gr√¢ce √† un mod√®le de Machine Learning, et de stocker les r√©sultats pour une analyse ult√©rieure. Le projet repose sur **Apache Kafka** pour garantir la robustesse et la scalabilit√© du transport des donn√©es.



[Image of Data Pipeline Architecture Diagram]


## üìÇ Architecture et Structure du Code

Le projet est d√©coup√© en composants ind√©pendants (Microservices pattern) :

### 1. Ingestion et Streaming (Kafka Producer)
* **`kafkaproducer.ipynb` / `streamindata.py`** : Ces scripts se connectent √† l'API de Reddit. Ils extraient les nouveaux posts/commentaires en temps r√©el et les publient ("produce") dans un **Topic Kafka**.
* Utilisation de la librairie `kafka-python` pour l'envoi des messages JSON.

### 2. Moteur de Machine Learning (Model Training)
Avant de traiter le flux, un mod√®le a √©t√© entra√Æn√© sur un dataset historique :
* **`trainindata.ipynb`** : Pr√©paration et nettoyage des donn√©es d'entra√Ænement.
* **`sentiment_model.ipynb`** : Entra√Ænement du mod√®le de classification (NLP). Le mod√®le est s√©rialis√© (sauvegard√©) pour √™tre r√©utilis√© par le consommateur sans r√©-entra√Ænement.

### 3. Traitement et Stockage (Kafka Consumer)
* **`kafkaconsumer.ipynb`** : Ce notebook √©coute ("consume") le Topic Kafka en continu.
    * Il re√ßoit les messages Reddit bruts.
    * Il applique le mod√®le de sentiment pr√©-entra√Æn√©.
* **`storedatainpost.ipynb` / `loaddata.py`** : Gestion de la persistance des donn√©es. Les r√©sultats (Texte + Sentiment Pr√©dit) sont ins√©r√©s dans une base de donn√©es **PostgreSQL**.

## üõ†Ô∏è Stack Technique

* **Langage :** Python
* **Streaming Platform :** Apache Kafka & Zookeeper
* **Data Source :** Reddit API (PRAW)
* **Machine Learning :** Scikit-learn (Pipeline NLP, Vectorization, Classification)
* **Base de Donn√©es :** PostgreSQL (via `psycopg2` ou SQLAlchemy)

## üöÄ Guide de D√©marrage

Pour faire tourner le pipeline complet, suivez cet ordre d'ex√©cution :

1.  **Infrastructure :** D√©marrez vos services Zookeeper et Kafka (localement ou via Docker).
2.  **Base de donn√©es :** Assurez-vous que votre instance PostgreSQL est active et que les tables sont cr√©√©es.
3.  **Mod√®le :** Ex√©cutez `sentiment_model.ipynb` une fois pour g√©n√©rer et sauvegarder le fichier du mod√®le `.pkl`.
4.  **Consumer :** Lancez `kafkaconsumer.ipynb`. Il restera en attente de messages.
5.  **Producer :** Lancez `kafkaproducer.ipynb`. Le flux de donn√©es commencera √† traverser le syst√®me.

## üìä Fonctionnalit√©s Cl√©s

* **Temps R√©el :** Latence minimale entre la publication sur Reddit et l'analyse.
* **D√©couplage :** L'utilisation de Kafka permet de s√©parer la collecte (Producer) de l'analyse (Consumer).
* **Persistance :** Archivage structur√© des analyses dans une base relationnelle SQL.

---

## üá¨üáß English Summary

**Project:** Real-Time Reddit Sentiment Analysis with Kafka

**Goal:** Build a streaming data pipeline to fetch Reddit comments, analyze their sentiment using ML, and store the results in a database.

**Key Features:**
* **Kafka Architecture:** Implements a Producer (fetching Reddit data) and a Consumer (processing data) decoupled via Kafka topics.
* **NLP Model:** Custom Machine Learning model trained to classify text sentiment, integrated directly into the consumption loop.
* **PostgreSQL Integration:** Processed data is automatically stored in a SQL database for reporting.

**Tech Stack:** Python, Apache Kafka, PostgreSQL, Scikit-Learn.
